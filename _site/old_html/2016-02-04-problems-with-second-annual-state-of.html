What is the relationship at your company between DEV and QA like:<br /><ul><li><b>Artist</b> and <b>Art Critic</b>?</li><li>Or <b>Writer</b> and <b>Copyeditor</b>?</li></ul><br />According to Sauce Labs' <i>Second Annual 'State of Testing Survey'</i> released a few days ago, testers are more than likely to find the latter instead of the former.<br /><br />Why? In a word: <b>Agile</b>.<br /><br /><a name='more'></a><br /><br /><h3>What is Waterfall and Agile?</h3><br />Back when I first started my career in the software testing industry, we used what was known as the <b>Waterfall Method</b> to develop software. &nbsp;Teams of business analysts figured out the requirements and design the architecture of the product, who would hand it off to the designers who would hand their work to the developers, who would then send their finished work to the testers. Some companies looped in the testers in their planning meetings, involving them into the meetings. Some did not, making the tester's life quite interesting.<br /><br />That changed with the <b>Agile Software Development Methodology</b>. Testers, developers, and designers scoped out, as a team, how to implement every two weeks the business requirements the product owner suggested.<br /><br /><ul><li>Read more about Agile on <a href="/2015/01/agile-software-development.html" target="_blank">my blog</a>&nbsp;&nbsp;</li></ul><br /><h3>Agile Pros and Cons</h3><br />Agile Software Development became a mixed blessing for testers:<br /><ul><li><b>PROS</b>: Testers are now a valued member of the software development team.</li><li><b>CONS</b>: No time to carefully draft and execute suites of regression test plans, verifying that adding the new stuff that the old stuff didn't break.&nbsp;</li></ul><br />Just before releasing an updated version of the web application, I'd lead the QA team in executing the extensive browser testing suite I drafted. We'd use a mouse and keyboard to point-and-click their way through the series of tests again, and again, and again, and again with Chrome, Firefox, Microsoft Internet Explorer version and all its versions from IE8 to IE10. It would be a very tedious one or two weeks.<br /><br />Automated testing fixes this: Develop an automated test suite for the various browsers and platforms using tools such as Selenium WebDriver<br /><br /><ul><li><b>PROS</b>: Goodbye manual regression test suite!</li><li><b>CONS</b>: Hello configuring all the browser and platforms in your testing environment!</li></ul><br /><h3>How Sauce Labs Helps Agile</h3><br />Sauce Labs provides automation developers with an online suite of browsers, operating systems, and mobile devices where they can execute their automated tests. Sauce Labs is very active in the testing community from running their own testing blog, <a href="http://sauceio.com/">SauceIO</a>  and sponsoring software testing podcasts such as <i>Test Talks with Joe Colantonio</i> at <a href="http://joecolantonio.com/testtalks/">http://joecolantonio.com/testtalks/</a>.<br /><br /><h3>Sauce Labs and the 'State of Testing'</h3><b>Press Release</b>: <i>Sauce Labs Releases Second Annual ‘State of Testing’ Survey Results</i><br /><a href="https://saucelabs.com/press-room/press-releases/sauce-labs-releases-second-annual-state-of-testing-survey-results">https://saucelabs.com/press-room/press-releases/sauce-labs-releases-second-annual-state-of-testing-survey-results</a><br /><br /><blockquote class="tr_bq"><i>"SAN FRANCISCO – Jan. 26, 2016 - Sauce Labs, Inc., provider of the world’s largest cloud-based platform for automated testing of web and mobile applications, today announced the results of an independent survey titled, <a href="https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf" target="_blank">'Testing Trends in 2016: A Survey of Software Professionals'</a>. The survey, conducted by Dimensional Research, reveals that while 88 percent of organizations say they have adopted agile development, only one in five have fully implemented the five best software testing practices typically associated with a mature agile development process.</i><br /><br /><i>"The report, commissioned by Sauce Labs, represents the company’s second annual 'State of Testing' research report, and is designed to better understand current trends in delivering high quality web and mobile applications. Covering topics such as agile software development, modern testing techniques, Continuous Integration (CI), and cross-browser test coverage, the report stems from a survey conducted in December 2015". </i>(Read <a href="https://saucelabs.com/press-room/press-releases/sauce-labs-releases-second-annual-state-of-testing-survey-results" target="_blank">MORE</a>)</blockquote><br /><b>Whitepaper</b>: <i>Testing Trends in 2016: A Survey of Software Professionals</i><br /><br />The document, "<i>Testing Trends in 2016: A Survey</i>", is at <a href="https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf.">https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf.</a> (12 pages)<br /><br /><center><embed height="500" src="https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf" type="application/pdf" width="400"></embed></center><br /><h3>About the Whitepaper</h3><br />Dimensional Research interviewed 520 "software professionals responsible for the quality of web applications. The goal of this global survey was to understand current trends in testing online environments. Certain questions were repeated from a similar survey conducted to the same audience one year ago to allow for trend analysis".<br /><br />Two things jumped out at me after reading Sauce Labs' survey:<br /><br /><ul><li>Agile is the way to go for us Software Quality Assurance Engineers, since "86% report development and QA teams think of themselves as partners"</li><li>Sauce Labs has a darned high bar with what they think of as "embracing Agile".</li></ul><div><br /></div><div>Once you have an automated test suite, you can set up:</div><div><ul><li>A small sample of tests to run against new code as soon as developers check in their code.&nbsp;</li><li>A larger set of tests to run against hourly builds to a testing environment that is similar to production, but with the dependencies scaled back.&nbsp;</li><li>Just before a release to the production environment, the code can then be deployed and tested against an exact clone of the production environment. &nbsp; &nbsp;</li></ul></div>Think of it as a bug filtration system. First you perform a sanity check. Then you &nbsp;putting the code through its initial paces. Then you test it on a close a simulation on the live environment we can get.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-wfINruaejtI/VrLecZbvIiI/AAAAAAAAK4E/VTsi68t2eGY/s1600/deployment.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="264" src="https://4.bp.blogspot.com/-wfINruaejtI/VrLecZbvIiI/AAAAAAAAK4E/VTsi68t2eGY/s640/deployment.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>According to the <a href="https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf" target="_blank">Survey</a>, people do weekly builds, but they wish for daily.</i></td></tr></tbody></table><br />If you have the automated deployment and testing system uses a Release Engineer who examines the results of the test before deployment, it is <b>Continuous Development</b>. If it goes right on through to Production, in is <b>Continuous Integration</b>.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-BFr8AdR9iLE/VrLdD_Y10tI/AAAAAAAAK34/vSK-0I3L4jw/s1600/How%2BHas%2BTesting%2BChanged.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="256" src="https://4.bp.blogspot.com/-BFr8AdR9iLE/VrLdD_Y10tI/AAAAAAAAK34/vSK-0I3L4jw/s640/How%2BHas%2BTesting%2BChanged.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>Taken from Sauce Labs '<a href="https://saucelabs.com/resources/white-papers/sauce-labs-state-of-testing-report-2016.pdf" target="_blank">State of Testing</a>' Report</i></td></tr></tbody></table><br /><br />As far as I knew, Continuous Integration was a supplement to the Agile Development Process. Agile was about co-workers being able to take charge of their own work schedule, contributing to a team.<br /><br /><div style="text-align: -webkit-center;"></div><div style="text-align: right;"><span style="font-size: large;"><b>"Individuals and interactions&nbsp;over processes and tools.</b></span></div><div style="text-align: right;"><span style="font-size: large;"><b>Working software&nbsp;over comprehensive documentation.</b></span></div><span style="font-size: large;"><b></b></span><br /><div style="text-align: right;"><span style="font-size: large;"><b>Customer collaboration&nbsp;over contract negotiation.</b></span></div><span style="font-size: large;"><b></b></span><br /><div style="text-align: right;"><span style="font-size: large;"><b>Responding to change&nbsp;over following a plan.&nbsp;</b></span><br /><span style="font-size: large;"><b>That is, while there is value in the items on&nbsp;<span style="font-size: large;"><b>the right,&nbsp;</b></span></b></span><br /><span style="font-size: large;"><b><span style="font-size: large;"><b>we value the items on the left more"</b></span>.</b></span></div><span style="font-size: large;"><b></b></span><br /><div style="text-align: right;">- The <a href="http://www.agilemanifesto.org/" target="_blank">Agile Manifesto</a>&nbsp;(2001)</div><br /><br />... Agile isn't about any particular <i>toolset</i>. It's about a <b>mindset</b>.<br /><br />This is why I found the survey results a bit odd. The survey claims that there are certain Agile best practices that they believe should be followed to achieve "agile testing maturity". Some points, I agree with. Some, not so much. The survey found:<br /><br /><ul><li><i>"23% fix bugs right away</i></li><li><i>"24% iterate small testable requirements rather than waiting for features to be completed</i></li><li><i>"26% have more automated testing than manual</i></li><li><i>"77% of the development and QA teams communicate in real-time</i></li><li><i>"86% report development and QA teams think of themselves as partners"</i></li></ul><div>With Agile, DEV and QA are equal partners -- finally! We are on the same team. We attend the same quick ten minute morning meetings that we call scrum... just a quick huddle to talk about what we worked on yesterday, what we were working on today, and if there are any roadblocks.&nbsp;</div><div><br /></div><h3>Bugs Should Be Fixed Right Away, Else It Ain't Agile?</h3><div><br /></div><div>But why the heck should all bugs be fixed right away? Who determined this was a <i>"best practice"</i> in Agile?<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-eOnMTDcuDOI/VrLf33gsz9I/AAAAAAAAK4Q/OgKYQnat8-s/s1600/bugs%2Bfixed.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="195" src="https://2.bp.blogspot.com/-eOnMTDcuDOI/VrLf33gsz9I/AAAAAAAAK4Q/OgKYQnat8-s/s400/bugs%2Bfixed.png" width="400" /></a></div><br /></div><div><br /></div><div>When triaging a bug, it is important to measure two quantities to perform a risk assessment:</div><div><ul><li>A) <b>Severity</b>: How disastrous to the business would it be if the bug happened? (Score 1 to 5)</li><li>B) <b>Frequency</b>: What are the chances of the bug happening? &nbsp;(Score 1 to 5)</li></ul><div>A * B = Risk Score. This determines the<b> priority</b>. &nbsp;</div></div><div><br /></div><div>If there is a very slim chance of a disastrous bug happening, or there is a bug that always happens with, say, by a small subset of the number of users of the web or mobile app, it shouldn't get a high priority. Yes, we should record the exact steps to reproduce the problem. Yes, we should get the system conditions when the bug happened. But there may be bigger fish to fry. Not all bugs can get fixed immediately. It completely depends on the team's bandwidth, and the risk score of the bug.&nbsp;</div><div><br /></div><h3>Testing Should Be Highly Automated, Else It Ain't Agile?</h3><div><br /></div><div>I hate to break it to Sauce Labs, since I love how they are supporting our testing community, but I think they are completely wrong on this point.&nbsp;</div><div><br /></div><div>Automation is not the end-all-and-be-all. It can't be. The code that is an automated test should be treated like production code. It takes time to analyze it, refactor it, clean it up, and have a peer review process as some type of code review.&nbsp;</div><div><br /></div><div>When new functionality is built into a product, we need a nimble team of manual testers to spearhead the testing process, brainstorming right along with the developers, exploring all technical aspects of what is being built. As the developers are asking <i>"How the heck do a BUILD this?"</i> the QA Team needs to ask <i>"How the heck do I TEST this?". </i>The automated testers can also ask, <i>"... And how do I automated these tests?"</i> getting the information straight from the sources, but I think it is a mistake to start developing code when the manual test team is still attempting to decide on an approach.&nbsp;</div><div><br /></div><div>As an automation developer, I believe it is my job to take the largest pain points away from the manual test team. What is the most mundane, routine tests that are lengthy, boring, and repetitive, thus the most prone to human error? The answer to that question should determine what should be automated.&nbsp;</div><div><br /></div><div>Automation can not solve everything: It can't tell if the User Interface is intuitive or not, or if it is well designed, or what the User Experience is like. An automated test can only find if A, B, or C is working. A trained manual tester can find the unknowns using user profiles, knowledge of the system, and instinctive hunches and guesses to test the system.&nbsp;</div><div><br /></div><div>&nbsp; &nbsp;</div><div><span style="background-color: white; color: #222222; font-family: &quot;calibri&quot;; font-size: 15.4px; line-height: 21.56px;">-T.J. Maher</span><br /><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;">&nbsp;Sr. QA Engineer, Fitbit</i><br /><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;">&nbsp;Boston, MA</i><br /><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;"><br /></i><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;">// Automated tester for [ 11 ] month and counting!</i><br /><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;"><br /></i><i style="background-color: white; color: #222222; font-family: Calibri; font-size: 15.4px; line-height: 21.56px;">Please note: 'Adventures in Automation' is a personal blog about automated testing. It is not an official blog of&nbsp;<a href="http://www.fitbit.com/" style="color: #888888; text-decoration: none;" target="_blank">Fitbit.com</a>.&nbsp;</i></div><script>  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)   })(window,document,'script','//www.google-analytics.com/analytics.js','ga');    ga('create', 'UA-73377959-1', 'auto');   ga('send', 'pageview');  </script>